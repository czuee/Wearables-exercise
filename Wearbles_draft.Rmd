---
title: "Wearables_draft"
author: "Czuee Morey"
date: "8/6/2019"
output: 
  html_document: 
    keep_md: yes
---

# Wearables activity prediciton
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r pckgs, include=FALSE}
library(ggplot2)
library(caret)
library(dplyr)
library(gridExtra)
library(grid)
library(FactoMineR)
library(corrplot)
library(doParallel)
```

## Business Question

#### What are we trying to predict?

The classe variable which consists of 5 categories defining how well the exercises were carried out by 6 participants. We have a number of sensor measurements that can be used to predict this variable.

#### What type of problem is it?

This is a multivariate prediction problem in which we have to build a model. Describe how you built your model, how you used cross validation, what you think the expected out of sample error is, and why you made the choices you did. You will also use your prediction model to predict 20 different test cases.

#### What type of data do we have? 
The data is in csv format. It presents a header row with the column names. The predictor variable is categorical.


## Read the files

```{r}
training <- read.csv("../pml-training.csv", header = TRUE, na.strings = c("NA", "", "#DIV/0!"))
testing  <- read.csv("../pml-testing.csv", header = TRUE, na.strings = c("NA", "", "#DIV/0!"))

dim(training); dim(testing)
```
I cleaned up the missing values, NAs and #DIV/0! while loading the files. There are a lot of missing values 19,000+, so imputation is not possible. 

##Exploratory data analysis

### Lets study the response variable
The response variable is classe. According to the text,
*Six young health participants were asked to perform one set of 10 repetitions of the Unilateral Dumbbell Biceps Curl in five different fashions: exactly according to the specification (Class A), throwing the elbows to the front (Class B), lifting the dumbbell only halfway (Class C), lowering the dumbbell only halfway (Class D) and throwing the hips to the front (Class E).

Read more: http://groupware.les.inf.puc-rio.br/har#ixzz5vuWJGhXP*

```{r}
qplot(training$classe, ylab = "Frequency" , xlab = "Class", main = "Exercise accuracy")
```
Class A which is the exercise done correctly is the most frequent. 

### Understanding the variables 

The publication mentioned on the website has details about the features.

- Euler angles (roll, pitch and yaw)
- For Euler angles of each of 4 sensor, eight features calculated: mean, variance, standard deviation, max, min, amplitude, kurtosis and skewness (`r 3*4*8` features)
-  raw accelerometer, gyroscope and magnetometer readings, for each sensor (`r 3*4` features)
- 6 users


```{r}
descr <- dlookr::describe(training)
descr[which(descr$skewness > 10), ]
```

- Many of the variables have lots of NAs. For exampe, kurtosis_yaw_belt does not have any significant values. 
- The #DIV/0! are probably computation errors. Also, the variables where DIV/0 occur do not have other values and hence imputation is not possible.
- Readings for each of the individuals were taken only at a certain timepoint. The raw time point part2 is uniformly distributed, while part 1 only has specific timepoints.
- Some of the variables have high skewness & kurtosis
- Time of doing exercise and user names should not be important to make a prediction for the exercise and hence can be removed.

### Feature Selection & preprocessing

Remove columns with NAs, and the users, timestamps which are not important.

```{r}
trainproc <- training[colSums(!is.na(training)) > 0]
trainproc <- trainproc[ ,-c(1:7)]

```

```{r}
trainproc <- trainproc[sapply(trainproc, function(x) !any(is.na(x)))]
dim(trainproc)
```

PCA with selected features
```{r}
typeColor <- as.numeric(trainproc$classe)
preProc <- preProcess(trainproc[,-53],method="pca",pcaComp=10)
trainPC <- predict(preProc, trainproc[,-53])

xyplot(PC1 ~ PC2, data = trainPC, groups =  typeColor, auto.key = list(columns = 5)) #Colored by classe
xyplot(PC1 ~ PC2, data = trainPC, groups =  as.numeric(training$user_name),auto.key = list(columns = 6)) #Colored by user name

```

The data seems to clearly cluster according to the different users. However, there also seems to be some clustering according to the classe variable. These patterns can be picked up using various training methods.

## Modeling

### Validation set
Creating a validation set to test out of sample error once I find suitable models.
```{r}
inTrain <- createDataPartition(trainproc$classe, p = 0.8, list = FALSE)

trainNum <- trainproc[inTrain,]
validation <- trainproc[-inTrain,]
dim(trainNum);dim(validation)
```

### Training on various predictors
#### Random Forest
```{r cache=TRUE}
set.seed(123)
registerDoParallel(4) 
getDoParWorkers()

my_control <- trainControl(method = "cv", # for "cross-validation"
                           number = 3, # number of k-folds
                           savePredictions = "final",
                           allowParallel = TRUE)

fit1 <- train(classe ~ ., 
              data = trainNum,
              method = c("rf"),
              trControl = my_control,
              )

fit1
```
The in-sample accuracy for random forest is 0.988. 

```{r}
fit1$finalModel
```

In-sample error rate is 1.2% (1-accuracy). OOB estimate of  error rate is 0.57%

Plot of random forest accuracy with predictors.
```{r}
plot(fit1, log = "y", lwd = 2, main = "Random forest accuracy", xlab = "Predictors", 
    ylab = "Accuracy")
```

Variable importance in the randomforest model
```{r}
randomForest::varImpPlot(fit1$finalModel)
```

#### Linear Discriminant analysis
```{r  cache=TRUE}
fit2 <- train(classe ~ ., 
              data = trainNum,
              method = c("lda"),
              trControl = my_control,
              )

fit2
```

Accuracy for LDA is much lower at 70.5%. In-sample error rate is 29.5%.

#### XG Boost Tree
```{r  cache=TRUE}
set.seed(123)
registerDoParallel(4) 
getDoParWorkers()
fit3 <- train(classe ~ ., 
              data = trainNum,
              method = c("xgbTree"),
              trControl = my_control,
              )
max(fit3$results$Accuracy)

```

Both random forest and xgbtree give an accuracy of ~0.99 on the training set. Let's see how they perform on the validation set.

```{r}
mat <- xgboost::xgb.importance(feature_names = colnames(trainNum),model = fit3$finalModel)
xgboost::xgb.plot.importance (importance_matrix = mat[1:20]) 
```


### Out-of-sample accuracy & error
```{r}

valpred1 <- predict(fit1, newdata = validation)
valpred2 <- predict(fit2, newdata = validation)
valpred3 <- predict(fit3, newdata = validation)

confusionMatrix(valpred1, validation$classe)
#confusionMatrix(valpred2, validation$classe)
confusionMatrix(valpred3, validation$classe)

```

xgbtree has the best prediction accuracy on the validation dataset of 0.996, as well as better per-class  statistics. Random forest is a close second.

#### Expected Out-of sample error in percent
```{r}
accuracy.rf <- sum(valpred1 == validation$classe)/length(valpred1)
accuracy.xgb <- sum(valpred3 == validation$classe)/length(valpred3)

out.sample.error.rf <- (1 - accuracy.rf)*100
out.sample.error.xgb <- (1 - accuracy.xgb)*100

print(out.sample.error.rf , digits = 3)
print(out.sample.error.xgb , digits = 3)

```


## Test set prediction
```{r}
test.xgb <- predict(fit3, newdata = testing)

test.xgb
```


#Conclusion

The dataset had a number of features alongwith the categorical response variable classe. The features with NA values were removed alongwith features like user and time which should not have an impact on our response variable. PCA with these features show that the classe variables segregate according to these features. It was interesting to note that there was clear segregation based on the users (even though users were not included as a feature). This indicates that characteristics of usage by different users could be detected by the sensors.

Random forest, XGBTree and LDA models were trained on the data using default parameters and 3-fold cross-validation. RF & XGBtree had the best performance with ~99% accuracy. The out-of-sample error rate estimated on the validation data was 0.54% and 0.43% for xgbtree & Rf respectively.

Prediction on the test set was done using XGB tree model.